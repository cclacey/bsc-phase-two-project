{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Real Estate Project\n",
    " **Authors:** CeCe Lacey, Warren Umbach, Kyle Vosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and Business Problem\n",
    "We set out to investigate the King County, Washington real estate market in 2015 in order to create a model for our clients. We sought to create a model that would best predict price of a home so that our clients could use it to help price homes they are buying or selling. We used linear regression to create a model using 15 features of 21,000 homes. these features were both discrete(categorical) and continous as well as internal home features as well as features of the surrounding neighborhood. Our final model is accurately predicting home prices with an average error of 148,000 dollars. We are confident in the success of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Process\n",
    "\n",
    "This notebook goes through our entire process for data cleaning, finding features, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "## Data Cleaning \n",
    "\n",
    "### Housing Data\n",
    "\n",
    "In the first step, we investigated our data columns so that we were aware of what we had. This process was done in another notebook called [firstmodel](./firstmodel.ipynb). In this notebook, we looked for missing values in each column. The basement_sqft column had an object data type and on further inspection, there was one question mark in it. That value was changed to a null and then the whole column was made a float. Then, the null value was replaced with the mean of the whole column. \n",
    "\n",
    "Next, we looked at a histogram of the price and saw that it was very skewed. We decided to drop 20 of the highest priced homes which was about 0.1% of the data.\n",
    "\n",
    "The null values in the next columns, yr_renovated, waterfront, and view, were replaced with zeros so that they would effect the model. We decided to make a new column called reno that is a zero or one indicating whether the house has been renovated or not.\n",
    "\n",
    "After that, we made three new columns called day, month, and year from the date column and then dropped the date column. We also had to make those columns into floats. Additionally, we used the month column to make another categorical feature called season.\n",
    "\n",
    "### Median Income by Zipcode\n",
    "\n",
    "We wanted to include some outside data to enhance our model so we went to IPUMS' (Integrated Public Use Microdata Series) National Historic Geographic Information System where we were able to retrieve data showing U.S. median income by zip code in 2015. Knowing that home prices are influenced by income levels we thought this wouuld be a helpful feature to add.\n",
    "\n",
    "In [this notebook](./NEEDTOMOVECECEFILE.ipynb) We culled this data down to just the 93 King County zip codes in our data. We turned this data into a dictionary to map it onto our cleaned dataframe and create a new column median_by_zip.\n",
    "\n",
    "### Nearest Whole Foods\n",
    "\n",
    "The other outside data we choose to incorporate was proximity to Whole Foods. Whole Foods are notoriously expensive grocery stores to shop at so we can only assume that people living in homes located near them can afford said expensive groceries and likely live in a more expensive home. \n",
    "\n",
    "We gathered a dataset from Kaggle that had the latitude and longitude for all the Whole Foods in the United States in 2017. We found the Whole Foods that were just in our King County and used haversine to find the distance from each of our houses to each of the Whole Foods.\n",
    "\n",
    "Then we created a column in our cleaned dataframe called closest_wf where we put the distance in miles of the closest Whole Foods from each house. \n",
    "\n",
    "## Data Understanding\n",
    "In order to determine what features we wanted to use in our model we needed to dig in and investigate their relationships with each other as well as with price. We ultimately combined some features in a variety of different ways, added the two new features from outside data, and dropped features that were not supporting our model. \n",
    "\n",
    "## Feature Engineering\n",
    "Next up was a little bit of feature engineering. In the notebook called [interactions](./interactions.ipynb), we looked at many combinations of features in order to tranform our data by multiplying or dividing feaures. We found a few features that were correlated and when combined had a big effect on our model such as multiplying sqft_living times grade and squaring the number of bathrooms.\n",
    "\n",
    "We added these to our cleaed_df so that the group could work from it as well as a few more engineered features after digging deeper into some interactions with the new Whole Foods and income data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "We looked at many interactions between variables that are shown in the interactions notebook. We picked the best ones and then dropped the individual features to prevent colinearity. We also made a barplot of the weights of each feature and took out the very lowest ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Names and descriptions for Our Final Model\n",
    "* **Price** -  Price is prediction target\n",
    "* **bedrooms** -  Number of Bedrooms/House\n",
    "* **bathrooms_sq** -  Number of bathrooms/House squared\n",
    "* **sqft_lot** -  Square footage of the lot\n",
    "* **waterfront** - 1 or 0 the house has a view to a waterfront or not\n",
    "* **condition** - How good the condition is ( Overall )\n",
    "* **sqrt_sqft_above** - Square root of square footage of house apart from basement\n",
    "* **sqft_basement** - Square footage of the basement\n",
    "* **reno** - 1 or 0 has the house been renovated\n",
    "* **lat** - Latitude coordinate\n",
    "* **long** - Longitude coordinate\n",
    "* **sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbors\n",
    "* **sqft_lot15** - The square footage of the land lots of the nearest 15 neighbors\n",
    "* **int_WFlivgrd** - Interaction of distance to nearest Whole Foods, square footage of house, and grade\n",
    "* **median_by_zip** - Median income of zip code where house is located\n",
    "* **view** - House has been viewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/cleaned_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a70aa1e129c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Reading in all of the data that we found from external sources and joining them all together.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/cleaned_df'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_med\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/mediandf.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_wf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/Closest_WF'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/cleaned_df'"
     ]
    }
   ],
   "source": [
    "#Reading in all of the data that we found from external sources and joining them all together.\n",
    "df = pd.read_csv('../data/cleaned_df',index_col=0)\n",
    "df_med = pd.read_csv('../data/mediandf.csv',index_col=0)\n",
    "df_wf = pd.read_csv('../data/Closest_WF',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the id column and then making all of the engineered features.\n",
    "df.drop('id',axis=1,inplace=True)\n",
    "df = df.join(df_med)\n",
    "df = df.join(df_wf)\n",
    "#waterfront time grade\n",
    "df['sqrt_long_times_WF'] = np.sqrt(abs(df['long'] * df['Closest_WF']))\n",
    "df['yr_built_times_grade'] = df['yr_built'] * df['grade']\n",
    "df['water_times_grade'] = df['waterfront'] * df['grade']\n",
    "df['sqrt_sqft_above'] = np.sqrt(abs(df['sqft_above']))\n",
    "df['int_WFlivgrd'] = np.sqrt(abs(df['sqft_living_times_grade'])) * np.sqrt(abs(df['Closest_WF']))\n",
    "# ('sqft_living_times_grade', 'Closest_WF', 0.798)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping 200 (1%) more of the most expensive houses to get a more normal price distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows = df['price'].sort_values(ascending=False)[:200]\n",
    "df.drop(drop_rows.index,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model packages and fit the basic model\n",
    "#for testing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#adding some more model testing\n",
    "from sklearn.model_selection import KFold\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#scoring\n",
    "#feature elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "#scoring\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#trying some polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Names and descriptions for features used in our final model.\n",
    "* **Price** -  Price is prediction target\n",
    "* **bedrooms** -  Number of Bedrooms/House\n",
    "* **bathrooms_sq** -  Number of bathrooms/House squared\n",
    "* **sqft_lot** -  Square footage of the lot\n",
    "* **waterfront** - 1 or 0 the house has a view to a waterfront or not\n",
    "* **condition** - How good the condition is ( Overall )\n",
    "* **sqrt_sqft_above** - Square root of square footage of house apart from basement\n",
    "* **sqft_basement** - Square footage of the basement\n",
    "* **reno** - 1 or 0 has the house been renovated\n",
    "* **lat** - Latitude coordinate\n",
    "* **long** - Longitude coordinate\n",
    "* **sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbors\n",
    "* **sqft_lot15** - The square footage of the lots of the nearest 15 neighbors\n",
    "* **int_WFlivgrd** - Interaction of distance to nearest Whole Foods, square footage of house, and grade\n",
    "* **median_by_zip** - Median income of zip code where house is located\n",
    "* **view** - House has been viewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting which categorical and continuous features to use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining which values are categorical or not\n",
    "#'bedrooms','floors','condition','zipcode','season','view'\n",
    "categoricals = ['bedrooms','waterfront','view','condition','reno','median_by_zip']\n",
    "\n",
    "continuous = ['price','sqft_lot','sqrt_sqft_above','sqft_basement','lat','long',\n",
    "              'sqft_living15','sqft_lot15',\n",
    "              'bathrooms_sq','int_WFlivgrd']\n",
    "\n",
    "df_cont = df[continuous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one hot encode the categorical features and combine them with the continuous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode the categoricals\n",
    "df_ohe = pd.get_dummies(df[categoricals])\n",
    "#combine the data into one frame\n",
    "preprocessed = pd.concat([df_cont, df_ohe],axis=1)\n",
    "#make feature set by dropping price\n",
    "X = preprocessed.drop('price',axis=1)\n",
    "\n",
    "#make target set\n",
    "y = preprocessed['price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing different scalers by making a function to take the output of scalers and return the score for multiple scalers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function applies scaling to the features depending on the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale here\n",
    "def scales(scaler,X):\n",
    "    #suppress warnings\n",
    "    import warnings\n",
    "    from sklearn.exceptions import DataConversionWarning\n",
    "    \n",
    "    warnings.filterwarnings(action='ignore', category=RuntimeWarning)\n",
    "                        \n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "#     from sklearn.preprocessing import minmax_scale\n",
    "    '''choose which scaler to use. Pass the name and features to scale'''\n",
    "    \n",
    "    if scaler == 'standard':\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        standard = StandardScaler().fit_transform(X)\n",
    "        choice = standard\n",
    "\n",
    "    elif scaler == 'minmax':\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        minmax = MinMaxScaler().fit_transform(X)\n",
    "        choice = minmax\n",
    "    elif scaler == 'maxabs':\n",
    "        from sklearn.preprocessing import MaxAbsScaler\n",
    "        maxabs = MaxAbsScaler().fit_transform(X)\n",
    "        choice = maxabs\n",
    "    elif scaler == 'robust':\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        robust = RobustScaler().fit_transform(X)\n",
    "        choice = robust\n",
    "    elif scaler == 'norm':\n",
    "        from sklearn.preprocessing import Normalizer\n",
    "        norm = Normalizer().fit_transform(X)\n",
    "        choice = norm\n",
    "    elif scaler == 'quant':\n",
    "        from sklearn.preprocessing import QuantileTransformer\n",
    "        quant = QuantileTransformer().fit_transform(X)\n",
    "        choice = quant\n",
    "    elif scaler == 'power':\n",
    "        from sklearn.preprocessing import PowerTransformer\n",
    "        power = PowerTransformer().fit_transform(X)\n",
    "        choice = power\n",
    "    elif scaler == 'none':\n",
    "        choice = X\n",
    "        \n",
    "    choice = pd.DataFrame(choice)\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the selected model is trained and where the train test split is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_train(scaled_X,y,n_feats,estimator):\n",
    "\n",
    "    #make train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, random_state=2)\n",
    "    #use the best features\n",
    "    #selector = RFE(estimator, n_features_to_select=n_feats, step=1)\n",
    "    #selector = selector.fit(X_test, y_test)\n",
    "    estimator = estimator.fit(X_test, y_test)\n",
    "    return estimator, y_train, y_test, X_test, X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function scores the model in a few different ways. It gets predictions for the train and test set and then uses those for root mean square error, r2 score, and cross validation. The r2 score is mostly ignored, but we optimized for the lowest error while keeping the train and test error difference minimal. I am using KFold for the cross validation with five splits and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(estimator,y_train,y_test,X_test,X_train,n_splits=5):\n",
    "    '''Takes estimator, y_train, y_test, X_test, X_train, n_splits\n",
    "        n_splits is optional'''\n",
    "    from sklearn.metrics import r2_score\n",
    "    #get predicted values\n",
    "    y_hat_train = estimator.predict(X_train)\n",
    "    y_hat_test = estimator.predict(X_test)\n",
    "    #r2 scoring\n",
    "    split_score = r2_score(y_test, y_hat_test)\n",
    "    r2_score = split_score.mean()\n",
    "    #RMSE scoring\n",
    "    RMSE_test = mean_squared_error(y_test,y_hat_test,squared=False)\n",
    "    RMSE_train = mean_squared_error(y_train,y_hat_train,squared=False)\n",
    "    RMSE_diff = abs(RMSE_test - RMSE_train)\n",
    "    # RMSE_test,RMSE_train\n",
    "    #cross val scoring using KFold splits\n",
    "    cv = KFold(n_splits=n_splits,shuffle=True)\n",
    "    cv_score = cross_val_score(estimator,X_test,y_test,scoring='r2',cv=cv)\n",
    "    cv_scores = cv_score.mean()\n",
    "    #estimator coefs to plot\n",
    "    coefs = estimator.coef_ \n",
    "    return round(r2_score,3),round(RMSE_diff,3),round(cv_scores,3),round(RMSE_test,3),round(RMSE_train,3),coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the model error for 10 bins sizes and then plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bin(est,X,y,bins=10):\n",
    "    groups = pd.qcut(y,bins)\n",
    "    frame = pd.DataFrame(groups)\n",
    "    frame['error'] = est.predict(X) - y\n",
    "    frame.columns = ['bins','error']\n",
    "    frame = frame.groupby('bins').mean()\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "    #y = abs(coef_hist[1].values)\n",
    "\n",
    "    y = frame['error'].values\n",
    "    labels = frame.index\n",
    "    sns.barplot(y=y,x=labels,ax=ax)\n",
    "    plt.xticks(rotation=45);\n",
    "    plt.title('Error by Bin');\n",
    "    plt.ylabel('Error in Dollars')\n",
    "    plt.xlabel('Price Bins in Dollars')\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last function that chooses the model and prints out the scores. It also returns all variables so that they can be used for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#set to degree 2, 3 is too slow\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import tree\n",
    "def get_scores(X=X):\n",
    "    scalers = ['standard']\n",
    "    estimators = ['linreg']\n",
    "    for estimator in estimators:     \n",
    "            if estimator == 'linreg':\n",
    "                est = LinearRegression(n_jobs=4)\n",
    "            elif estimator == 'sgd':\n",
    "                est = SGDRegressor()\n",
    "            elif estimator == 'ridge':\n",
    "                est = Ridge(alpha=0.4,max_iter=1000)\n",
    "            elif estimator == 'lasso':\n",
    "                est = Lasso(alpha=0.1,max_iter=1000,tol=10000)\n",
    "            elif estimator == 'kn':\n",
    "                est = KNeighborsRegressor(n_neighbors=20,n_jobs=4)\n",
    "            elif estimator == 'tree':\n",
    "                est = tree.DecisionTreeRegressor()\n",
    "            for scaler in scalers:\n",
    "                scaled_X = scales(scaler,X)\n",
    "                model_trains = model_train(scaled_X,y,10,est)\n",
    "                estimator = model_trains[0]\n",
    "                y_train = model_trains[1]\n",
    "                y_test = model_trains[2]\n",
    "                X_test = model_trains[3]\n",
    "                X_train = model_trains[4]\n",
    "                r2_score, RMSE_diff, cv_score, RMSE_test, RMSE_train, coefs = model_score(estimator,\n",
    "                                       y_train,\n",
    "                                       y_test,\n",
    "                                       X_test,\n",
    "                                       X_train,\n",
    "                                       5)\n",
    "                frame = plot_bin(estimator, X_test, y_test)\n",
    "                print(f'Model used: {est}\\n'\n",
    "                      f'Scaler used: {scaler}\\n'\n",
    "                      f'Model scores are:\\n' f'r2_score: {r2_score}\\n' \n",
    "                      f'RMSE difference: {RMSE_diff}\\n'\n",
    "                      f'Test RMSE: {RMSE_test}\\n'\n",
    "                      f'Train RMSE: {RMSE_train}\\n'  \n",
    "                      f'Cross Val Scores: {cv_score}\\n\\n') \n",
    "    return estimator,scaler,r2_score, RMSE_diff, cv_score, RMSE_test, RMSE_train,coefs, frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I call the score function and get the estimator coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator,scaler,r2_score, RMSE_diff, cv_score, RMSE_test, RMSE_train,coefs, frame = get_scores(X)\n",
    "\n",
    "coefficients = list(zip(X.columns,coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the absolute error for each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['error'] = round(abs(frame['error']))\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a few variables to get a sense of the relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(x='sqft_living15', y=\"price\", hue='reno',data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the coefficient importance was plotted to be sure that all features were contributing sufficiently to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_hist = pd.DataFrame(coefficients)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "len(coef_hist[1].values),len(coef_hist[0].values)\n",
    "\n",
    "#y = abs(coef_hist[1].values)\n",
    "\n",
    "y = coef_hist[1].values\n",
    "labels = coef_hist[0].values\n",
    "ax = sns.barplot(y=y,x=labels)\n",
    "plt.xticks(rotation=45);\n",
    "plt.title('Coefficient Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The model is scored based on the root mean square error of the train and test sets,\n",
    "a cross validation, and the error of the test set using 10 price bins from 81 thousand dollars to 2 million dollars.\n",
    "\n",
    "* Train Test Split Error Scores: \n",
    "Test Root Mean Squared Error: 149120.457\n",
    "Train Root Mean Squared Error: 147543.401\n",
    "\n",
    "* Cross Validation Score: 0.717\n",
    "\n",
    "* Quantile Test:\n",
    "Most error is between 854 thousand and 2 million. The error in all other bins is around 30 thousand dollars down to 5.5 thousand dollars for one bin.\n",
    "\n",
    "With our final model we were able to decrease our RMSE by over 30,000 dollars from our baseline model. Our overall RMSE came down from over 180,000 dollars to under 150,000 dollars. Our model also performed very well across 9 of 10 quantiles that we tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In conclusion, two of our engineered features most impacted our model. The highest being the square root of sq_ft_above with a coefficient of nearly 200,000 and the second highest being the interaction between square foot living, grade, and nearest Whole Foods being -150,000. This interaction is negative because the smaller the distance to Whole Foods, the greater the price of the home. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "To further the accuracy of our model we would more deeply investigate aspects of our homes and neighborhoods. Initial thoughts would be to take more census data into account around income and schools, accessibility to municipal services, and additional features of the home such as size of driveway, type of heating, or existence of porch. Finally, we would like to create a model with price per square foot as the target as that would leave much less room for error and could help us better hone in on the best model possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
